<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数值稳定性：合理的权重初始和激活函数</title>
      <link href="/2025/04/14/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%EF%BC%9A%E5%90%88%E7%90%86%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2025/04/14/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%EF%BC%9A%E5%90%88%E7%90%86%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h1><p>考虑到如下有d层的神经网络：<br>$\mathbf{h}^t&#x3D;f_t(\mathbf{h}^{t-1})\quad\mathrm{and}\quad y&#x3D;\ell\circ f_d\circ…\circ f_1(\mathbf{x})$</p><p>其中t代表神经网络层数。</p><p>计算损失$\ell$关于参数$W_t$的梯度为：<br>$\frac{\partial\ell}{\partial\mathbf{W}^t}&#x3D;\frac{\partial\ell_0}{\partial\mathbf{h}^d}\frac{\partial\mathbf{h}^d}{\partial\mathbf{h}^{d-1}}…\frac{\partial\mathbf{h}^{t+1}}{\partial\mathbf{h}^t}\frac{\partial\mathbf{h}^t}{\partial\mathbf{W}^t}$</p><p>这里的h都是代表向量，而我们知道，向量关于向量的导数是一个矩阵，因此上述公式除去首尾可以看作是d-t次的矩阵乘法</p><h1 id="数值稳定性的常见问题：梯度爆炸、梯度消失"><a href="#数值稳定性的常见问题：梯度爆炸、梯度消失" class="headerlink" title="数值稳定性的常见问题：梯度爆炸、梯度消失"></a>数值稳定性的常见问题：梯度爆炸、梯度消失</h1><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><p>这里的梯度爆炸很好理解，举个例子，加入我神经网络的每层的梯度都是1.5，那么在经历100层之后，这个梯度就会来到$4*10^{27}$，这就有可能会导致python浮点数达到一个上限。</p><h3 id="梯度爆炸问题"><a href="#梯度爆炸问题" class="headerlink" title="梯度爆炸问题"></a>梯度爆炸问题</h3><ul><li>值超出值域<ul><li>对16位浮点数尤为严重($6<em>10^{-5}-6</em>10^{4}$)</li></ul></li><li>对学习率敏感<ul><li>学习率太大-&gt;大参数值-&gt;更大的梯度</li><li>学习率太小-&gt;模型训练无法得到进展</li><li>我们可能需要在训练过程中不断调整学习率<br>  <em><strong>（学习率相当于步长，梯度相当于收敛方向）</strong></em></li></ul></li></ul><h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><p>和梯度爆炸类似，假如我神经网络每层的梯度都是0.8，那么在经历100层之后，这个梯度就会来到$2*10^{-10}$，同样会导致python浮点数上限</p><h3 id="梯度消失问题"><a href="#梯度消失问题" class="headerlink" title="梯度消失问题"></a>梯度消失问题</h3><ul><li>梯度值变成0<ul><li>对16位浮点数尤为严重</li></ul></li><li>训练没有进展<ul><li>不管如何训练</li></ul></li><li>对底部层尤为严重<ul><li>仅仅在顶部层训练好</li><li>无法让神经网络更深（和浅神经网络没有区别）</li></ul></li></ul><h1 id="如何让训练更加稳定"><a href="#如何让训练更加稳定" class="headerlink" title="如何让训练更加稳定"></a>如何让训练更加稳定</h1><ul><li>目标：让梯度值在合理范围内</li><li>常用方法：将乘法变加法<ul><li>ResNet, LSTM</li></ul></li><li>归一化<ul><li>梯度归一化，梯度裁剪</li></ul></li><li>合理的权重初始和激活函数</li></ul><h1 id="合理的权重初始和激活函数"><a href="#合理的权重初始和激活函数" class="headerlink" title="合理的权重初始和激活函数"></a>合理的权重初始和激活函数</h1><ol><li><p>合理的权重初始</p><ul><li><p>将每层的输出和梯度都看作是随机函数，让他们的均值和方差都保持一致。</p><ul><li>权重初始：在合理值区间里随机初始函数。因为在训练开始的时候容易有数值不稳定，例如远离最优解的地方损失函数表面可能很复杂，最优解附近的表面可能比较平。<strong>（使用$N(0, 0.01)$对小网络可能没啥影响，但是对深度神经网络就行不通了）</strong><br>  这边具体的介绍可以去看李沐老师的动手学深度学习数值稳定性部分，以MLP为例（假设没有激活函数），从数学角度推导了初始方差如何选择即保证正向方差:<br>  $$n_{t-1}*Var[{W_{i,j}}^t]&#x3D;1$$<br>  其中$n_{t-1}$为输入维度即上一层的神经元数量<br>  反向部分这边就不过多介绍，与正向情况类似。</li></ul></li><li><p>Xavier初始化</p><p>  由于难以同时满足$n_t\gamma_t&#x3D;1$和$n_{t-1}\gamma_t&#x3D;1$，因此Xavier使得$\gamma_t(n_{t-1}+n_t)&#x2F;2&#x3D;1$即$\gamma_t&#x3D;2&#x2F;(n_{t-1}+n_t)$。<br>  例如对当前层进行权重初始时选择正态分布$\mathcal{N}\left(0,\sqrt[]{2&#x2F;(n_{t-1}+n_t)}\right)$或者均匀分布$\mathscr{U}\left(-\sqrt{6&#x2F;(n_{t-1}+n_t)},\sqrt{6&#x2F;(n_{t-1}+n_t)}\right)$。</p></li></ul></li><li><p>激活函数</p><p> 前面提到过让训练更稳定需要让每层的输出和梯度的均值和方差保持一致，激活函数所采用的思想也是一样。<br> 检查常用的激活函数，本文以sigmoid,tanh和ReLu函数为例，对这三个激活函数使用泰勒展开：<br> $$\mathrm{sigmoid}(x)&#x3D;\frac{1}{2}+\frac{x}{4}-\frac{x^3}{48}+O(x^5) \<br> \tanh(x)&#x3D;0+x-\frac{x^3}{3}+O(x^5) \<br> relu(x)&#x3D;0+x\quad\mathrm{for~}x\geq0$$<br> 我们可以知道，通常在神经网络的训练过程中权重W的值通常在0附近徘徊，因此一个合适的激活函数应当是当x趋近于0时，$f(x)&#x3D;x$。由泰勒展开可知，当x&#x3D;0时，tanh函数和ReLu函数均为0，而sigmoid函数值为$1&#x2F;2$。因此，我们需要调整sigmoid函数：<br> $$4\times\mathrm{sigmoid}(x)-2$$</p></li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>当数值过大或过小都会导致数值问题</li><li>常发生在深度模型中，因为其会对n个数累乘</li><li>合理的权重初始值和激活函数的选取可以提高数值稳定性</li></ul><h1 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h1><p>本文为笔者的学习分享，如有错误，欢迎指正修改。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数值稳定性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂七杂八1</title>
      <link href="/2025/04/11/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB1/"/>
      <url>/2025/04/11/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB1/</url>
      
        <content type="html"><![CDATA[<p>感觉今天发生的事情还挺多的，简单记录下吧。</p><p>在前面三天尝试了凯圣王的三分化训练计划，emmm怎么说呢，感觉肌肉确实有酸痛感，但是背部感觉不太好，可能是因为其中的罗马尼亚硬拉没太敢上太大重量，目前还是在学习动作模式的阶段，慢慢来吧。</p><p>今天摆烂了一整天，论文一点没搞，摆烂的感觉真爽哈哈哈。明天还是要继续好好学习啊。不过今天确实干了不少事，早上就直接跳过吧…大学生都是没有早上的…下午看着之前的next主题越看越不顺眼，缩写又折腾了大半个下午，把主题换成butterfly了，一下子高大上多了哈哈哈。大概五点那会突然很想打球，好久都没有打球了其实。自从上了大三后可能两三个月才打一次，哎，挺感慨的，高中的时候每天是想逃课打球，现在反而却不想打了，究竟是啥原因呢。感觉篮球对我的吸引力没有以前那么强了，也有可能我喜欢的本来就不是打球，而是和好朋友一起打球把。</p><p>晚上回来后刷到表白墙在找家教，我嘞个豆250一个小时，直接动心了，立马把自己简历发送过去了。不过还要收信息费，第一次经历这种，还是挺担心的，钱被骗了是小事，腰子被噶了就完了哈哈哈，到时候试课还是得找个人和我一起…</p><p>这边弄的差不多了之后尝试了德州扑克，还真挺好玩的，以后回家的活动项目又多了一个（手动狗头）</p><p>今天也就这么多事，不过相比过去的大学生活还是丰富了许多，自从开始选择保研后都没好好体验过大学生活，感觉自己和周围人已经格格不入了，哎。</p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello My Blog</title>
      <link href="/2025/04/10/hello-world/"/>
      <url>/2025/04/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>大家好！这是我的第一篇博客！</p><p>简单介绍一下吧，本人目前就读于上海某高校数据科学与大数据技术专业，2026年毕业，每天的日常就是吃饭，睡觉，学习，健身。由于我走的是保研这条路，所以现在还在写论文，研究方向为水文和人工智能的交叉应用。不过现在的水平只能将人工智能当作一个工具（调包大师），闲下来的时候也有在系统的学习AI方面的知识（统计学习方法和李沐老师的动手学深度学习）。</p><p>在后续的这段日子，我会在博客上更新一些日常的杂七杂八感想和学习记录。博客刚搭起来所以还有点毛坯，后面会慢慢优化的。我的邮箱也放在博客上，欢迎志同道合的人一起交流。</p><p>梦想是去$lambda$组读博，共勉。</p>]]></content>
      
      
      <categories>
          
          <category> Hello Blog </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
